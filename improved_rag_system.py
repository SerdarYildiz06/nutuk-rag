#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from rank_bm25 import BM25Okapi
import numpy as np
import re
import sys
import os
from typing import List, Tuple
import pickle

class ImprovedNutukRAGSystem:
    """Ä°yileÅŸtirilmiÅŸ Nutuk belgeleri iÃ§in RAG sistemi"""
    
    def __init__(self, model_name="qwen2:latest", rebuild_db=False):
        """RAG sistemini baÅŸlatÄ±r"""
        print("ğŸš€ Ä°yileÅŸtirilmiÅŸ Nutuk RAG Sistemi baÅŸlatÄ±lÄ±yor...")
        
        # Ayarlar
        self.chunk_size = 300  # Daha kÃ¼Ã§Ã¼k chunk boyutu
        self.chunk_overlap = 50
        self.persist_directory = "improved_rag_chroma_db"
        self.bm25_path = "bm25_index.pkl"
        
        # Daha iyi embedding modelini yÃ¼kle
        print("ğŸ“š GeliÅŸmiÅŸ embedding modeli yÃ¼kleniyor...")
        embedding_model_name = "sentence-transformers/all-mpnet-base-v2"  # Daha gÃ¼Ã§lÃ¼ model
        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)
        print("âœ… Embedding modeli yÃ¼klendi")
        
        # PDF'i yeniden iÅŸle veya mevcut DB'yi yÃ¼kle
        if rebuild_db or not os.path.exists(self.persist_directory):
            print("ğŸ”„ PDF yeniden iÅŸleniyor...")
            self._rebuild_database()
        else:
            print("ğŸ—ƒï¸ Mevcut ChromaDB yÃ¼kleniyor...")
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            # BM25 indeksini yÃ¼kle
            if os.path.exists(self.bm25_path):
                with open(self.bm25_path, 'rb') as f:
                    self.bm25, self.bm25_docs = pickle.load(f)
                print("âœ… BM25 indeksi yÃ¼klendi")
            else:
                print("âš ï¸ BM25 indeksi bulunamadÄ±, yeniden oluÅŸturuluyor...")
                self._create_bm25_index()
        
        print("âœ… ChromaDB yÃ¼klendi")
        
        # Ollama modelini yÃ¼kle
        print(f"ğŸ¤– Ollama modeli ({model_name}) yÃ¼kleniyor...")
        self.llm = OllamaLLM(model=model_name)
        print("âœ… Ollama modeli yÃ¼klendi")
        
        # GeliÅŸmiÅŸ prompt template'i oluÅŸtur
        self.prompt_template = ChatPromptTemplate.from_template("""
Sen AtatÃ¼rk'Ã¼n Nutuk eseri konusunda uzman bir tarihÃ§isin. AÅŸaÄŸÄ±daki belgeler kullanÄ±larak soruyu detaylÄ± ve aÃ§Ä±k bir ÅŸekilde yanÄ±tla. 

Ã–nemli kurallar:
- Belgelerde aÃ§Ä±k bir bilgi varsa, o bilgiyi tam olarak kullan
- Tarihleri, isimleri ve olaylarÄ± olduÄŸu gibi aktar
- Ã–zellikle sayfa numaralarÄ±nÄ± belirt
- MÃ¼mkÃ¼n olduÄŸunca detaylÄ± ve kapsamlÄ± yanÄ±t ver
- EÄŸer belgelerden tam yanÄ±t bulamÄ±yorsan, mevcut bilgileri kullanarak en iyi tahmini yap

Belgeler:
{context}

Soru: {question}

YanÄ±t: Verilen belgelere dayanarak,""")
        
        print("ğŸ‰ Ä°yileÅŸtirilmiÅŸ RAG sistemi hazÄ±r!")
    
    def _rebuild_database(self):
        """PDF'i yeniden iÅŸleyerek veritabanÄ±nÄ± oluÅŸturur"""
        pdf_path = "nutuk.pdf"
        if not os.path.exists(pdf_path):
            print(f"âŒ {pdf_path} bulunamadÄ±!")
            sys.exit(1)
        
        # PDF'i yÃ¼kle
        print("ğŸ“„ PDF yÃ¼kleniyor...")
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        
        # Daha kÃ¼Ã§Ã¼k chunk'lara bÃ¶l
        print("âœ‚ï¸ Belgeler kÃ¼Ã§Ã¼k parÃ§alara bÃ¶lÃ¼nÃ¼yor...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", " ", ""]
        )
        
        chunks = text_splitter.split_documents(documents)
        print(f"âœ… {len(chunks)} kÃ¼Ã§Ã¼k parÃ§a oluÅŸturuldu")
        
        # ChromaDB'ye kaydet
        print("ğŸ’¾ ChromaDB'ye kaydediliyor...")
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        # BM25 indeksini oluÅŸtur
        self._create_bm25_index()
        
        print("âœ… VeritabanÄ± oluÅŸturuldu")
    
    def _create_bm25_index(self):
        """BM25 keyword arama indeksini oluÅŸturur"""
        print("ğŸ” BM25 keyword arama indeksi oluÅŸturuluyor...")
        
        # TÃ¼m dokÃ¼manlarÄ± al
        all_docs = self.vectorstore.get()
        texts = all_docs['documents']
        metadatas = all_docs['metadatas']
        
        # Metinleri tokenize et
        tokenized_texts = [self._tokenize_turkish(text) for text in texts]
        
        # BM25 indeksini oluÅŸtur
        self.bm25 = BM25Okapi(tokenized_texts)
        self.bm25_docs = [(text, meta) for text, meta in zip(texts, metadatas)]
        
        # Kaydet
        with open(self.bm25_path, 'wb') as f:
            pickle.dump((self.bm25, self.bm25_docs), f)
        
        print("âœ… BM25 indeksi oluÅŸturuldu")
    
    def _tokenize_turkish(self, text: str) -> List[str]:
        """TÃ¼rkÃ§e metin iÃ§in tokenization"""
        # KÃ¼Ã§Ã¼k harfe Ã§evir
        text = text.lower()
        
        # TÃ¼rkÃ§e karakterleri normalize et
        text = text.replace('Ã§', 'c').replace('ÄŸ', 'g').replace('Ä±', 'i')
        text = text.replace('Ã¶', 'o').replace('ÅŸ', 's').replace('Ã¼', 'u')
        
        # Sadece harf ve rakam bÄ±rak
        text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
        
        # Tokenize et
        tokens = text.split()
        
        # Stop words'leri kaldÄ±r (temel TÃ¼rkÃ§e stop words)
        stop_words = {
            've', 'bir', 'bu', 'o', 'ÅŸu', 'de', 'da', 'den', 'dan', 'ile', 'iÃ§in',
            'ne', 'ki', 'ya', 'yada', 'veya', 'ama', 'fakat', 'ancak', 'lakin',
            'gibi', 'kadar', 'sonra', 'Ã¶nce', 'Ã¼zere', 'doÄŸru', 'karÅŸÄ±', 'raÄŸmen'
        }
        
        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
        
        return tokens
    
    def semantic_search(self, query: str, k: int = 5) -> List[Document]:
        """Semantic similarity search"""
        results = self.vectorstore.similarity_search(query, k=k)
        return results
    
    def keyword_search(self, query: str, k: int = 5) -> List[Tuple[str, dict, float]]:
        """BM25 keyword search"""
        query_tokens = self._tokenize_turkish(query)
        scores = self.bm25.get_scores(query_tokens)
        
        # En yÃ¼ksek skorlu k adet sonucu al
        top_indices = np.argsort(scores)[::-1][:k]
        
        results = []
        for idx in top_indices:
            if scores[idx] > 0:  # Sadece 0'dan bÃ¼yÃ¼k skorlarÄ± al
                doc_text, metadata = self.bm25_docs[idx]
                results.append((doc_text, metadata, scores[idx]))
        
        return results
    
    def hybrid_search(self, query: str, k: int = 6) -> List[Document]:
        """Hibrit arama: semantic + keyword"""
        print(f"ğŸ” Hibrit arama yapÄ±lÄ±yor: {query}")
        
        # Semantic arama
        semantic_results = self.semantic_search(query, k=k//2)
        
        # Keyword arama
        keyword_results = self.keyword_search(query, k=k//2)
        
        # SonuÃ§larÄ± birleÅŸtir
        all_results = []
        
        # Semantic sonuÃ§larÄ± ekle
        for doc in semantic_results:
            all_results.append(doc)
        
        # Keyword sonuÃ§larÄ±nÄ± Document objesine Ã§evir ve ekle
        for text, metadata, score in keyword_results:
            doc = Document(page_content=text, metadata=metadata)
            # Duplicate kontrolÃ¼
            is_duplicate = False
            for existing_doc in all_results:
                if existing_doc.page_content == doc.page_content:
                    is_duplicate = True
                    break
            if not is_duplicate:
                all_results.append(doc)
        
        print(f"âœ… {len(all_results)} benzersiz sonuÃ§ bulundu")
        return all_results[:k]
    
    def rerank_results(self, query: str, documents: List[Document]) -> List[Document]:
        """SonuÃ§larÄ± yeniden sÄ±ralar"""
        query_lower = query.lower()
        query_tokens = set(self._tokenize_turkish(query))
        
        scored_docs = []
        for doc in documents:
            score = 0
            content_lower = doc.page_content.lower()
            content_tokens = set(self._tokenize_turkish(doc.page_content))
            
            # Exact match bonus
            if query_lower in content_lower:
                score += 10
            
            # Token overlap bonus
            overlap = len(query_tokens.intersection(content_tokens))
            score += overlap * 2
            
            # Length penalty (Ã§ok kÄ±sa metinleri cezalandÄ±r)
            if len(doc.page_content) < 50:
                score -= 2
            
            scored_docs.append((doc, score))
        
        # Skora gÃ¶re sÄ±rala
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, score in scored_docs]
    
    def search_documents(self, query: str, k: int = 6):
        """GeliÅŸmiÅŸ arama: hibrit + reranking"""
        # Hibrit arama
        results = self.hybrid_search(query, k=k*2)  # Daha fazla sonuÃ§ al
        
        # Reranking
        if results:
            results = self.rerank_results(query, results)
        
        # En iyi k sonucu dÃ¶ndÃ¼r
        return results[:k]
    
    def generate_answer(self, question: str, context_docs: List[Document]) -> str:
        """LLM ile yanÄ±t Ã¼retir"""
        # Belgeleri sayfa numarasÄ±na gÃ¶re sÄ±rala
        context_docs = sorted(context_docs, key=lambda x: x.metadata.get('page', 0))
        
        # Belgeleri tek bir metin haline getir
        context = "\n\n".join([
            f"Sayfa {doc.metadata.get('page', '?')}: {doc.page_content}" 
            for doc in context_docs
        ])
        
        # Prompt'u oluÅŸtur
        formatted_prompt = self.prompt_template.format(
            context=context, 
            question=question
        )
        
        print("ğŸ¤– Ollama ile yanÄ±t Ã¼retiliyor...")
        response = self.llm.invoke(formatted_prompt)
        return response
    
    def ask(self, question: str, k: int = 6):
        """Tam RAG iÅŸlemi: geliÅŸmiÅŸ arama + yanÄ±t Ã¼retme"""
        print(f"\n{'='*70}")
        print(f"â“ Soru: {question}")
        print(f"{'='*70}")
        
        # 1. GeliÅŸmiÅŸ arama
        context_docs = self.search_documents(question, k=k)
        
        if not context_docs:
            return "âŒ Ä°lgili belge bulunamadÄ±."
        
        # 2. Bulunan belgeleri gÃ¶ster
        print("\nğŸ“„ Bulunan belgeler:")
        for i, doc in enumerate(context_docs, 1):
            page = doc.metadata.get('page', '?')
            content_preview = doc.page_content[:150].replace('\n', ' ')
            print(f"  {i}. Sayfa {page}: {content_preview}...")
        
        # 3. LLM ile yanÄ±t Ã¼ret
        answer = self.generate_answer(question, context_docs)
        
        print(f"\nğŸ’¬ YanÄ±t:\n{answer}")
        print(f"\n{'='*70}")
        
        return answer

def main():
    """Ana fonksiyon - interaktif soru-cevap"""
    try:
        # KullanÄ±cÄ±ya seÃ§enek sun
        print("ğŸ”§ VeritabanÄ±nÄ± yeniden oluÅŸturmak ister misiniz? (y/n): ", end="")
        rebuild = input().strip().lower() in ['y', 'yes', 'evet', 'e']
        
        # RAG sistemini baÅŸlat
        rag = ImprovedNutukRAGSystem(rebuild_db=rebuild)
        
        print("\nğŸ¯ Ä°yileÅŸtirilmiÅŸ Nutuk RAG Sistemi hazÄ±r!")
        print("ğŸ’¡ Nutuk hakkÄ±nda soru sorabilirsiniz.")
        print("ğŸ’¡ Ã–zellikler: Hibrit arama, kÃ¼Ã§Ã¼k chunk'lar, reranking")
        print("ğŸ’¡ Ã‡Ä±kmak iÃ§in 'q' yazÄ±n.\n")
        
        while True:
            try:
                question = input("â“ Sorunuz: ").strip()
                
                if question.lower() in ['q', 'quit', 'exit', 'Ã§Ä±k']:
                    print("ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
                    break
                
                if not question:
                    continue
                
                # Soruyu yanÄ±tla
                rag.ask(question)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
                break
            except Exception as e:
                print(f"âŒ Hata: {e}")
                
    except Exception as e:
        print(f"âŒ Sistem baÅŸlatÄ±lamadÄ±: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
